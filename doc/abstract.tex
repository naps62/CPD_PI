\documentclass[9pt,twocolumn]{scrartcl}

\usepackage{ucs}
\usepackage[utf8x]{inputenc}
\usepackage[T1]{fontenc}
\usepackage[english]{babel}

%\usepackage{graphics}%	images other than eps

\usepackage[paper=a4paper,top=2cm,left=1.5cm,right=1.5cm,bottom=2cm,foot=1cm]{geometry}

\usepackage{relsize}%	relative font sizes

%\usepackage[retainorgcmds]{IEEtrantools}%	IEEEeqnarray

\usepackage{hyperref}

\usepackage{listings}
\usepackage{color}

%%%%%%%%%%%%%%%%
%  title page  %
%%%%%%%%%%%%%%%%
\titlehead{Universidade do Minho \hfill Parallel and Distributed Computing\\	Master's Degree in Informatics Engineering}

\title{Profiling + CUDA}

\subtitle{A finite volume case study from an industrial application\\(Extended Abstract)}%	TODO ó Naps, did lá o nome dest a merda!!

\author{Miguel Palhas \hfill--- \texttt{\smaller pg19808@alunos.uminho.pt}\\Pedro Costa \hfill--- \texttt{\smaller pg19830@alunos.uminho.pt}}

\date{Braga, January 2012}

\subject{Integrated Project}


%%%%%%%%%%%
%  Hacks  %
%%%%%%%%%%%

%	Paragraph with linebreak
\newcommand{\paragraphh}[1]{\paragraph{#1\hfill}\hfill

}


\begin{document}
\maketitle

\section{Introduction}
%\subsection{Contextualization}
%The Finite Volumes Method (FVM) is one of the three classical choices for solving PDEs\footnote{Partial Differential Equations: equations involving functions with more than one variable and their partial derivatives.} numerically, together with the Finite Difference Method (FDM) and the Finite Element Method (FEM).

%Although the FDM is the oldest, it requires a structured mesh, whilst that's not true for the other two, which rely on the integral form discretizations. In comparison with the FEM, the FVM can conserve the variables on a coarse mesh easily.

%The FDM is the oldest method and is based in the classical formal definition of a function's derivative in order to a parameter $x$ as the limit of that function's average rate as the difference between the two points tends towards zero ($\Delta x \rightarrow 0$).

%$${\left . u_{x} \right |}_{i} = u_{x} \left ( x_{i} \right ) = \lim_{\Delta x \rightarrow 0}{\frac{u \left ( x_{i} + \Delta x \right ) - u \left ( x_{i} \right )}{\Delta x}}$$

%Even if $\Delta x$ is not constant throughout the mesh, this forces the need for it to be structured on the FDM. The FEM and FVM overcome this by discretizing based on the integral form of the governing equations, therefore being able to handle complex geometries in multi-dimensional problems.

%TODO: FEM

%TODO: FVM

%TODO: fluids mechanics & pollution spreading


%\subsection{Motivation}% what we intend to achieve
This document analyzes an application which computes the evolution of a pollutant in a given environment, studying possibilities of optimization and/or parallelization.

The application calculates the pollutant spread area at variable moments during a given time interval. This pollutant spreads according to a partial differential equation which is solved using the Finite Volume Method.

The algorithm used by the program loops until the specified time interval is reached. At each iteration of this main loop, the flux of pollution is calculated in each edge (performed by a function named \texttt{compute\_flux}) and the mesh pollution levels are updated (performed by the \texttt{update} function).

Two attempts of parallelization are intended: one using shared memory on a multiprocessor system and one using a GPU.

The final aim is to have a qualitative and quantitive comparison of both versions against each other and the original sequential code, and an evaluation of possible bottlenecks and/or optimizable code parts.

%\subsection{Structure}% what will we do (list of actions), a.k.a. structure
The initial profiling of the application, including the methodology and conclusions, is presented in \autoref{sec:initprof}. In \autoref{sec:optm&para}, the initial profiling information is used to identify the parts of the algorithm/code which are better suited for optimization and/or parallelization. The implementation of a shared memory parallel version and a GPU version are explained in \autoref{sec:openmp}  and \autoref{sec:cuda}, respectively.

\section{Sequential}% why did we choose compute_flux & update to mess with
\label{sec:initprof}
The analysis of the sequential code starts with the generation of it's callgraph, from which it is possible to find the function where the program execution spends most of it's time. According to Amdahl's Law, this function should be the first to study for optimizations and/or parallelization, as it's where one can profit the most from speedup.

For the purpose of generating the call graph, the \texttt{Callgrind} tool\footnote{From the Valgrind suite.} was used. The information generated by this tool could then be visualized using \texttt{KCachegrind}.

Originally this reported 11\% of the execution time in the \texttt{compute\_flux} function and 8\% in the \texttt{update}, while the remaining\footnote{A very small percentage of the time is spent on other functions, which can therefore be ignored} 80\% of the time was spent with I/O (to build an animation). Since the goal is optimize the computation, the I/O calls were deactivated and the profiling tool reported 63\% of the time in the \texttt{compute\_flux} and the remaining time in the \texttt{update}.

\section{Optimization \& Parallelization}
\label{sec:optm&para}
As determined in \autoref{sec:initprof}, the best targets for optimization and/or parallelization are the \texttt{compute\_flux} and \texttt{update} functions, which will be the primary and secondary goals for this project, respectively (each parallelized version will be focusing on these functions by this order).

The first optimization possible in the \texttt{compute\_flux} function is the removal of the \texttt{Parameter.getDouble()} call. This was meant to retrieve the Dirichlet condition value from the parameter file, but being this a constant throughout the program execution, the value can be loaded in an early preparation stage and sent as an argument to \texttt{compute\_flux}. Additionally, moving this argument to a global variable can reduce the function call overhead.

Excessive dereferencing may be a common problem on both functions and is directly related to the FVLib library. Although the usage of pointers may greatly improve code readability, the increased memory accesses caused by deep levels of dereferencing tend to aggravate effects of any memory bottlenecks. With some adjustments to the library internal structures, the same or better readability might be achievable with fewer pointers and lower reference depth.

Lastly, to allow the parallelization in both intended approaches, the loops themselves have to be changed both in \texttt{compute\_flux} and \texttt{update}. The original code uses a non-standard iterator-like approach to loop through the elements of the mesh. While this works in sequential code, it won't work with conventional parallelization mechanisms, as there would be no knowledge of the iterations other than the data itself (which is not enough). This mechanisms requires index based accesses, or at least random-access iterators\footnote{OpenMP works with both, CUDA works naturally with indexes.}. Since the FVLib library implements index based accesses, the changes in the loops are trivial.

\subsection{Dependencies}
In the \texttt{compute\_flux} function a data dependency can be found which prevents parallelization. This dependency is found in the calculation of $\Delta t_{i}$ (the final value $\Delta t_{n}$ is returned by the function) and causes each iteration to depend on the previous one (the first iteration uses a preset value $\Delta t_{0}$).

Using mathematical operations this calculation can be removed from the loop and the dependency substituted by a calculation of the maximum computed velocity ($v_{max}$), which can be computed through a reduction.

The method of calculation through reduction is the best option to parallelize the calculation of iteration dependent associative binary operators ($\sum$,$\prod$,min,max). The goal of this method is to reduce the problem size as the computation advances: each thread picks two values, and in the next step only half the threads are needed, until only one is left with the final value

%To solve this question without compromising the parallelization, the \texttt{max} function is performed by each thread on it's iterations, storing the values in a common array (prepared at the beginning of the program). After the loop, this local maxima array is sequentially scanned to find the global maximum computed velocity. Although this imposes a limit on the number of threads (which would cause the scan to take longer), it represents a fair compromise.

The \texttt{update} function also holds a problem when thinking of parallelization. This function iterates through the edges of the mesh, and updates the values associated with the cells adjacent to those edges. Since each cell has multiple edges and the final value associated with a cell is the sum of the contributions from all it's edges, then multiple iterations might be changing this value simultaneosly.

It's possible to remove this dependency by changing the loop itself to iterate over each cell calculating the final value from all it's edges, instead of iterating over the edges. Considering that no edge data is changed, this completely removes de dependency and allows the parallelization of the function.

\section{OpenMP}
\label{sec:openmp}
\subsection{Implementation}
The main feature about OpenMP that makes it so interesting to create shared memory parallel code is that the program itself is almost equal to the sequential version, with the addition of the required directives and adaptation tweaks (mainly to solve concurrency problems).

Because there are no nested parallelizable loops in this code, the number of threads to be issued are set by default to the maximum number of threads supported by the hardware to be executing at any given time. %Despite that, this number is multiplied by a factor (1 by default) to allow experiences to be easily conducted using more threads. The goal of this approach is to achieve an effect similar to the one in GPUs: theoretically, issuing more threads than those the hardware can execute in parallel may improve efficiency by allowing any thread to step in while another is stalled on any resource.

\subsubsection{Flux Computation}
%As expected, the parallelization of the \texttt{compute\_flux} function is very straight-forward with the exception of the maximum computed velocity computation. This problem is discussed next, and is therefore ignored here.

To parallelize this function, a \texttt{parallel for} OpenMP directive is placed before the inner loop. Considering all the variables external to the parallel zone as shared, all the required variables must be explicitly made private (either with the \texttt{private({\textit{list}})} clause or by declaring the variables only inside the parallel zone).

%\paragraphh{\texttt{max} reduction}
%The implementation of the \texttt{max} function reduction is only available in \texttt{gcc} 4.7 (at the date, the standard version is 4.6).
%
%To solve this question without compromising the parallelization, the \texttt{max} function is performed by each thread on it's iterations, storing the values in a common array (prepared at the beginning of the program). After the loop, this local maxima array is sequentially scanned to find the global maximum computed velocity. Although this imposes a limit on the number of threads (which would cause the scan to take longer), it represents a fair compromise.

%\subsection{Profiling}
%%\subsubsection{PAPI framework}% why doesn't PAPI have class in the source FFS
%%To profile the resulting code with some detail, a framework was implemented to take advantage of C++ features (such as exceptions and objects) when using the PAPI library.
%%
%%This framework allows to group several events into predefined sets, as long as creating custom sets on the go. Given the changes required to profile OpenMP code, this framework greatly facilitates the library error handling, improving robustness and readability.
%%
%%The following sets are predefined: CPI (total instructions and cycles), Memory (load and store instructions), Flops/s (floating point operations and total cycles), L1 (first level cache accesses and misses), L2 (same for second level cache), Operational Intensity (RAM bytes accessed and Flops), Instructions Per Byte (RAM bytes accessed and total instructions), Multiplication/Addition Balance (FP total, multiplication and division instructions).
%
%%\subsubsection{Methodology}% scripts are cool, talk about them
%To perform measurements in the parallel code the PAPI library requires some adaptation. Although the library initialization and cleanup can (and should) be performed in the same places as the sequential code, each thread in a parallel zone must handle it's own event set. Additionally, when initializing the library, the thread support must be explicitly activated.
%
%Once in the parallel zone, each thread is responsible for creating it's own event set (either predefined or custom), starting it before the loop begins, stopping it after while collecting the results (which are stored in a shared array, like the local maxima), and removing the event set before the parallel zone ends. Outside, the global maximum computation is measured in the same way, and the final	 results are gathered sequentially.


\section{CUDA}
\label{sec:cuda}
\subsection{Data Structures}
For a GPU implementation, all the data structures had to be converted to a suitable format (using sequential memory positions instead of dereferencing). Since this was required from the start, it was also a chance to optimize memory for GPU computing, by using a structure of arrays instead of an array of structures, to take advantage of the GPU's coallesced memory accesses.

\subsection{Implementation}


\subsubsection{Flux Computation}
A total of 3 CUDA kernels were written for the implementation. The first one was to replace the \texttt{compute\_flux} function. This is a very straight-forward implementation, with no major changes to the original algorithm, aside from the fact that each CUDA thread handles a single element, instead of looping through the mesh.

%\paragraphh{\texttt{max} reduction}
%As previously explained, a reduction calculation was also required to allow for parallelization of the \texttt{compute\_flux} function. This is a widely used example in CUDA, and was implemented with the help of the NVidia samples.

\subsubsection{Mesh Update}
The \texttt{update} function was also converted to CUDA. This was not as interesting to parallelize in the OpenMP version, since it doesn't occupy the majority of the time. However, using the original CPU implementation would mean that for every iteration of the main loop, flux and polution data had to be copied back from the GPU to execute the update, and then copied again to GPU for the next iteration. This would eliminate any advantage of GPU parallelization, so this function had to be re-written, taking into consideration the data races of the original one.

\section{Conclusion}
In this document the original application code was analyzed and studied. The two key functions in the program execution were identified using the program's callgraph. Also, a major bottleneck was identified in the animation generation, which invalidated any computation optimization effort when activated.

Several problems were easily detected in early stages, along with parallelization obstacles. These problems were mostly related with the program's memory usage, and some even prevented parallelization. The identified obstacles, namely data dependences, were solved by manipulating the original code, only possible due to an increasingly better understanding of the algorithm. 

Two parallel versions of the application were implemented. The first version implemented the same code using the OpenMP directives to parallelize the most critical part of the program. The second version adapted the original code to perform all the required computations in a GPU using CUDA.

All versions still lack a formal profiling which would allow to identify bottlenecks and compare efficiency. Despite that, both of the implemented parallel versions showed a small but consistent speedup in multiple executions. While this shows that the application can be improved with parallelization, the differences classify the algorithm as non-scalable.

Further optimization efforts can be done to improve the obtained results. For the OpenMP implementation, these include extending the parallelization to the \texttt{update} function and changing the data structures. For the CUDA implementation, a revision of memory access efficency might be useful, as well as the usage of CUDA Streams to minimize idle time of the GPU. Also, for both implementations, changing how the FVLib library works, specifically how the data is accessed, could greatly improve the program efficiency specially with the animation generation.


%	Side Notes: OpenFVM.sourceforge.net

\begin{thebibliography}{12345}

%\bibitem[FVM]{finite}
%	Joaquim Peiró and Spencer Sherwin	\\
%	\textit{Finite Difference, Finite Element and Finite Volume Methods for Partial Differential Equations}	\\
%	Department of Aeronautics, Imperial College, London, UK	\\
%	\texttt{\smaller http://multiscale.emsl.pnl.gov/docs/finite.pdf}

\bibitem[OMP]{openmp}
	\textit{OpenMP Application Program Interface}	\\
	OpenMP Architecture Review Board	\\
	Version 3.1, July 2011

\bibitem[NVIDIA]{nvidia}
	\hfill\\
	\texttt{\smaller http://developer.nvidia.com/category/zone/cuda-zone}

\end{thebibliography}

\end{document}
